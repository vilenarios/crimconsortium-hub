# Testing Guide - Complete End-to-End Pipeline

## âœ… What's Been Implemented

All components for the Markdown archival pipeline are now complete and integrated:

### Backend (100% Complete)
1. âœ… Database schema with migration
2. âœ… Full-content scraper (`npm run scrape:full`)
3. âœ… Markdown generator (`npm run generate:markdown`)
4. âœ… Arweave uploader (`npm run upload:articles`)
5. âœ… Updated parquet export with `arweave_tx_id`

### Frontend (100% Complete)
1. âœ… ParquetDB queries include `arweave_tx_id`, `word_count`, `attachment_count`
2. âœ… ArticleDetail component fetches from Arweave
3. âœ… Fallback to local content if no TX ID
4. âœ… Displays Arweave badge and metadata

---

## ğŸ§ª Test the Complete Pipeline (10 Articles)

Follow these steps to test end-to-end with a small sample:

### Step 1: Scrape Full Content (Phase 1)

```bash
npm run scrape:full -- --limit=10
```

**Expected output:**
```
================================================================================
ğŸ“š Full Content Scraper - Markdown Archival Pipeline (Phase 1)
================================================================================

ğŸ—„ï¸  Initializing SQLite database...
ğŸ”„ Running database migration...
  Adding column: content_prosemirror
  Adding column: content_markdown
  ... (more columns)
âœ… Migration complete: Added 16 columns
âœ… Database initialized

ğŸ”Œ Connecting to PubPub API...
âœ… Connections established

ğŸ“Š Found 10 articles needing full content

[1/10]
ğŸ“„ Processing: article-slug-1
  â†’ Fetching metadata...
  â†’ Fetching full content...
  â†’ Converting to Markdown...
  â†’ Downloading 2 attachment(s)...
  â†’ Saving to database...
  âœ… Complete (5432 words, 2 attachments)

... (continues for 10 articles)

ğŸ“Š Scraping Complete
================================================================================
Total articles: 10
Processed: 10
Errors: 0
Time elapsed: 0.7 minutes
================================================================================
```

**What this does:**
- Fetches full ProseMirror content from PubPub
- Converts to Markdown
- Downloads PDFs and attachments
- Stores in SQLite with all metadata

---

### Step 2: Generate Markdown Files (Phase 2)

```bash
npm run generate:markdown -- --limit=10
```

**Expected output:**
```
================================================================================
ğŸ“ Markdown Article Generator - Archival Pipeline (Phase 2)
================================================================================

ğŸ“ Created output directory: C:\Source\crimconsortium-hub\data\articles-markdown
ğŸ—„ï¸  Initializing SQLite database...
âœ… Database connected

ğŸ“Š Found 10 articles needing Markdown generation

[1/10]
ğŸ“„ Processing: article-slug-1
  â†’ Generating Markdown...
  â†’ Saved: 45.2 KB
  âœ… Complete

... (continues for 10 articles)

ğŸ“Š Generation Complete
================================================================================
Total articles: 10
Generated: 10
Errors: 0
Time elapsed: 0.2 minutes
Output directory: C:\Source\crimconsortium-hub\data\articles-markdown
================================================================================
```

**What this does:**
- Generates standalone Markdown files with YAML frontmatter
- Includes metadata, abstract, full content, references, citations
- Saves to `data/articles-markdown/`

**Verify files were created:**
```bash
ls data/articles-markdown/
```

---

### Step 3: Test Upload (Dry Run - No Cost!)

```bash
npm run upload:articles -- --limit=10 --dry-run
```

**Expected output:**
```
================================================================================
â˜ï¸  Arweave Article Uploader - Archival Pipeline (Phase 3)
ğŸ” DRY RUN MODE - No actual uploads will be performed
================================================================================

ğŸ—„ï¸  Initializing SQLite database...
âœ… Database connected (dry run mode)

ğŸ“Š Found 10 articles needing upload

[1/10]
ğŸ“„ Processing: article-slug-1
  â†’ Uploading Markdown (45.2 KB)...
  âœ… Article uploaded: dry-run-abc123
  â†’ Uploading 2 attachment(s)...
  âœ… Attachment uploaded: article.pdf â†’ dry-run-def456
  âœ… Attachment uploaded: supplement.csv â†’ dry-run-ghi789
  âœ… Complete

... (continues for 10 articles)

ğŸ“Š Dry Run Complete
================================================================================
Total articles: 10
Uploaded: 10
Errors: 0
Total size: 2.4 MB
Estimated cost: $0.0024
Time elapsed: 0.5 minutes
================================================================================
```

**What this does:**
- Simulates uploads (no real Arweave transactions)
- Estimates file sizes and costs
- Validates all Markdown files exist

---

### Step 4: Export Metadata Parquet

```bash
npm run export
```

**Expected output:**
```
============================================================
SQLite â†’ Parquet Exporter
============================================================

ğŸ—„ï¸  Opening SQLite database...
ğŸ¦† Initializing DuckDB...
âœ… Connections established

ğŸ“¦ Found 10 unexported articles

ğŸ“Š Creating 1 batch file(s)...

  ğŸ“ Batch 1/1: 2025-10-28_batch-001
     Articles: 10
     Size: 0.08 MB
     âœ… Written

ğŸ“‹ Exporting metadata.parquet...

âœ… Metadata exported
   Articles: 3,721 (only 10 have full content)
   Size: 0.76 MB

ğŸ“Š Export Summary
============================================================
Batches created: 1
Total articles exported: 10
Total batch size: 0.08 MB
Metadata size: 0.76 MB
Total size: 0.84 MB
============================================================
```

**What this does:**
- Exports new articles to batch Parquet files
- Updates metadata.parquet with `arweave_tx_id` (NULL for now since dry-run)
- Creates `data/parquet/metadata.parquet` and `data/parquet/articles/`

---

### Step 5: Test the Vite App

```bash
# Copy parquet files to public directory for dev server
cp -r data/parquet/* public/data/

# Build the app
npm run build

# Run dev server
npm run dev
```

**Expected output:**
```
VITE v7.1.12  ready in 432 ms

âœ  Local:   http://localhost:5173/
âœ  Network: use --host to expose
âœ  press h + enter to show help
```

**Open browser:** http://localhost:5173/

**What to test:**
1. âœ… Homepage loads with 25 recent articles
2. âœ… Click an article (one of the 10 you processed)
3. âœ… Article detail page shows metadata
4. âœ… Since no real Arweave upload, it falls back to local content
5. âœ… Search works across title, abstract, keywords, authors

---

## ğŸš€ Full Production Run (After Testing)

Once 10-article test succeeds, run the full pipeline:

### 1. Scrape All Articles (~3-4 hours)

```bash
npm run scrape:full
```

### 2. Generate All Markdown (~10-15 min)

```bash
npm run generate:markdown
```

### 3. **IMPORTANT**: Set up Turbo SDK

Add to `.env`:
```bash
TURBO_PRIVATE_KEY='{"kty":"RSA","n":"...","e":"AQAB",...}'
```

Get your JWK from [ArConnect](https://www.arconnect.io/) or create a new Arweave wallet.

### 4. Upload to Arweave (~2-3 hours, ~$10)

```bash
# First, verify cost estimate
npm run upload:articles -- --dry-run

# If cost is acceptable, upload for real
npm run upload:articles
```

### 5. Export Final Metadata

```bash
npm run export
```

### 6. Deploy Everything

```bash
# Copy parquet to public
cp -r data/parquet/* public/data/

# Build app
npm run build

# Deploy to Arweave
npm run deploy
```

---

## ğŸ” Troubleshooting

### Issue: "Articles needing full content: 0"

**Solution:** Your database already has articles. Reset or mark them for re-scraping:
```sql
sqlite3 data/sqlite/crimrxiv.db "UPDATE articles SET full_content_scraped = 0"
```

### Issue: "TURBO_PRIVATE_KEY not found"

**Solution:** You need an Arweave wallet for uploads:
1. Install [ArConnect](https://www.arconnect.io/)
2. Create/import wallet
3. Export JWK
4. Add to `.env` as `TURBO_PRIVATE_KEY`

### Issue: "Markdown file not found"

**Solution:** Run Phase 2 (generate:markdown) before Phase 3 (upload):
```bash
npm run generate:markdown
```

### Issue: App shows "Article Not Found"

**Solution:**
1. Make sure you ran `npm run export`
2. Copy parquet files: `cp -r data/parquet/* public/data/`
3. Restart dev server: `npm run dev`

---

## ğŸ“Š Progress Tracking

### Check Database Status

```bash
sqlite3 data/sqlite/crimrxiv.db << 'EOF'
SELECT
  COUNT(*) as total,
  SUM(CASE WHEN full_content_scraped = 1 THEN 1 ELSE 0 END) as scraped,
  SUM(CASE WHEN markdown_generated = 1 THEN 1 ELSE 0 END) as markdown,
  SUM(CASE WHEN arweave_tx_id IS NOT NULL THEN 1 ELSE 0 END) as uploaded
FROM articles
WHERE is_latest_version = 1;
EOF
```

### Check Parquet Files

```bash
ls -lh data/parquet/
ls -lh data/parquet/articles/
```

### Check Markdown Files

```bash
ls -lh data/articles-markdown/ | wc -l
```

---

## âœ¨ What's Next?

After testing succeeds:

1. **Run full pipeline** for all 3,721 articles
2. **Upload to Arweave** (~$10 one-time cost)
3. **Deploy app** with updated metadata.parquet
4. **Set up ArNS** (Arweave Name System) for human-readable URL
5. **Celebrate!** ğŸ‰ You now have a permanent, decentralized archive

---

## ğŸ“š Related Documentation

- [MARKDOWN_ARCHIVAL_PIPELINE.md](./MARKDOWN_ARCHIVAL_PIPELINE.md) - Complete architecture
- [FULL_CONTENT_ARCHIVAL_PLAN.md](./FULL_CONTENT_ARCHIVAL_PLAN.md) - Original plan
- [PATTERN_GUIDE.md](./PATTERN_GUIDE.md) - Development patterns

---

## ğŸ†˜ Need Help?

If you encounter issues:

1. Check the console logs (each script is verbose)
2. Look at the database state (SQL queries above)
3. Verify file paths and permissions
4. Ensure `.env` has correct credentials

The pipeline is fully functional and ready to run! ğŸš€
